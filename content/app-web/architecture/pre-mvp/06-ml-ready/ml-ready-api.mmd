flowchart LR
  %% ML-Ready API Overview â€” with q_ml and metrics

  subgraph FE["Frontend (Next.js)"]
    fe_infer["call POST /infer (payload)"]
    fe_fb["call POST /feedback (label, note)"]
  end

  subgraph Edge["Edge Functions"]
    cors["CORS + rate limits"]
    infer["/infer"]
    feedback["/feedback"]
    cfg["read REMOTE_CONFIG\n{ ml_infer_enabled, model_endpoint_url }"]
    q_ml["q_ml (retry queue)"]
    logi["insert INFER_LOGS"]
    logf["insert FEEDBACK"]
  end

  subgraph Model["Model Layer"]
    stub["Stub rules (fallback)"]
    ext["Model API (future)"]
  end

  subgraph Cron["Self-healing jobs"]
    cron_ml["cron_ml_retry"]
  end

  subgraph KPIs["Metrics"]
    sr["infer_success_rate"]
    l95["latency_p95"]
    er["error_rate"]
  end

  %% Infer path
  fe_infer --> cors --> infer --> cfg
  infer -->|ml_infer_enabled == false| stub --> logi
  infer -->|ml_infer_enabled == true| ext
  ext -->|200| logi
  ext -->|5xx/timeout| q_ml
  q_ml --> cron_ml --> ext

  logi --> sr
  logi --> l95
  q_ml --> er

  %% Feedback path
  fe_fb --> cors --> feedback --> cfg
  feedback --> vfb["validate & strip PII"]
  vfb --> logf

  %% Responses
  logi --> r1["200 {predictions or stub}"]
  logf --> r2["202 {accepted:true}"]
  r1 --> fe_infer
  r2 --> fe_fb
